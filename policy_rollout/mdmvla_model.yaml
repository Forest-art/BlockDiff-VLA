wandb:
  entity: null
  resume: 'auto'

experiment:
    project: "BlockDiff-VLA"
    name: "mdmvla-demo"
    output_dir: "mdmvla-demo"

act_step: 10

model:
    framework: "mdmvla"
    vq_model:
        type: "magvitv2"
        vq_model_name: "showlab/magvitv2"

    showo:
        pretrained_model_path: "showlab/show-o-w-clip-vit-512x512"
        tuned_model_path: "./MDMVLA-action-10/checkpoint-170000"
        w_clip_vit: True
        vocab_size: 58498
        llm_vocab_size: 50295
        llm_model_path: './showlab/phi-1_5'
        codebook_size: 8192
        num_vq_tokens: 1024
        num_new_special_tokens: 10

    vla:
      num_view: 1

    gradient_checkpointing: True

dataset:
    gen_type: "pre"
    und_type: "captioning"
    params:
        batch_size: ${training.batch_size}
        shuffle_buffer_size: 1000
        num_workers: 32
        resolution: 512
        pin_memory: True
        persistent_workers: True

    preprocessing:
        max_seq_length: 576
        resolution: 512
        center_crop: False
        random_flip: False

training:
    clip_pad_tokens: False
    gradient_accumulation_steps: 1
    cond_dropout_prob: 0.0
    batch_size: 20
