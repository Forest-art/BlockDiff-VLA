wandb:
  entity: null
  resume: "auto"

experiment:
    project: "BlockDiff-VLA"
    tracker: "tensorboard"
    name: "BDVLA-Qwen3-elasticBD-action-10"
    output_dir: "BDVLA-Qwen3-elasticBD-action-10"
    save_every: 10000
    eval_every: 2500
    generate_every: 5000
    log_every: 50
    log_grad_norm_every: 500
    resume_from_checkpoint: "latest"

act_step: 10

model:
    framework: "bdvla"
    vq_model:
        type: "magvitv2"
        vq_model_name: "showlab/magvitv2"

    showo:
        load_from_showo: False
        w_clip_vit: True

        # Qwen3 base model path (HF repo id or local path)
        llm_model_path: "Qwen/Qwen3-1.7B"
        llm_backbone: "auto"
        trust_remote_code: True
        attn_implementation: "sdpa"

        # If true, train_upvla.py will infer llm_vocab_size from tokenizer
        # and auto-refresh vocab_size = llm_vocab_size + codebook_size + num_new_special_tokens + 1(mask).
        auto_set_llm_vocab_size: True

        # Placeholder values; auto_set_llm_vocab_size=true will refresh these at runtime.
        llm_vocab_size: 151936
        codebook_size: 8192
        num_new_special_tokens: 10
        vocab_size: 160139
        num_vq_tokens: 1024

    vla:
        num_view: 1

    gradient_checkpointing: True

block_diffusion:
    text_enabled: True
    action_enabled: False

    # Fallback fixed block size if elastic sampling is disabled.
    block_size: 32
    # Elastic block-size training: sample one size per sample per step.
    elastic_block_enabled: True
    block_size_candidates: [8, 16, 32, 64]

    mask_eps: 1.0e-3
    complementary_mask: True

    action_num_bins: 256
    action_conf_threshold: 0.90
    action_infer_enabled: False

    # Legacy rollout fallback knobs (used by old inference path).
    action_min_mask_ratio: 0.20
    action_max_mask_ratio: 0.70

dataset:
    gen_type: "future_view_prediction_w_action"
    und_type: "llava_tuning"
    combined_loader_mode: "min_size"
    params:
        train_pre_shards_path_or_url: "path to the preprocessed calvin training files"
        train_mmu_shards_path_or_url: "path to llava_tuning_665k_data"
        add_caption_prompt: True
        validation_prompts_file: "./validation_samples/predict_prompts.txt"
        shuffle_buffer_size: 1000
        num_workers: 8
        resolution: 512
        pin_memory: True
        persistent_workers: True

    preprocessing:
        max_seq_length: 576
        resolution: 512
        center_crop: False
        random_flip: False

optimizer:
    name: adamw
    params:
        learning_rate: 0.0001
        scale_lr: False
        beta1: 0.9
        beta2: 0.999
        weight_decay: 0.01
        epsilon: 1e-8

lr_scheduler:
    scheduler: "cosine"
    params:
        learning_rate: ${optimizer.params.learning_rate}
        warmup_steps: 1000

training:
    text_objective: "bd"
    clip_pad_tokens: False
    gradient_accumulation_steps: 4
    batch_size_pre: 4
    batch_size_mmu: 4
    mixed_precision: "bf16"
    enable_tf32: True
    seed: 10086
    max_train_steps: 200000
    overfit_one_batch: False
    cond_dropout_prob: 0.0
    min_masking_rate: 0.0
    label_smoothing: 0.0
    max_grad_norm: null
    guidance_scale: 0.0
    generation_timesteps: 12
    pre_coeff: 1.0
    act_coeff: 1.0
    mmu_coeff: 1.0

eval: False
mmu_image_root: "./validation_samples"
question: "Please describe this image in detail. *** Describe things you see in the image and point out their positions in this image."
max_new_tokens: 100
